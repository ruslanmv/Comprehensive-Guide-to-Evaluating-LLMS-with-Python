{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Guide to Evaluating Language Models (LLMs) with Python\n",
    "\n",
    "# Install necessary libraries\n",
    "# !pip install numpy pandas scikit-learn rouge-score nltk detoxify lm-eval matplotlib\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from detoxify import Detoxify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example dataset for evaluation\n",
    "gold_standard = [\n",
    "    {\"query\": \"What is 2 + 2?\", \"correct_answer\": \"4\"},\n",
    "    {\"query\": \"Who wrote Macbeth?\", \"correct_answer\": \"William Shakespeare\"},\n",
    "    {\"query\": \"What is the boiling point of water?\", \"correct_answer\": \"100째C\"}\n",
    "]\n",
    "\n",
    "model_outputs = [\n",
    "    {\"query\": \"What is 2 + 2?\", \"output\": \"4\"},\n",
    "    {\"query\": \"Who wrote Macbeth?\", \"output\": \"Charles Dickens\"},\n",
    "    {\"query\": \"What is the boiling point of water?\", \"output\": \"100째C\"}\n",
    "]\n",
    "\n",
    "texts = [\n",
    "    \"This is a friendly and respectful comment.\",\n",
    "    \"This is a hateful and offensive comment.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination Reduction Rate (HRR): 100.00%\n",
      "This score reflects the percentage of hallucinations corrected by the model. A higher value indicates fewer factual errors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Hallucination Reduction Rate (HRR)\n",
    "def calculate_hrr(baseline_outputs, validated_outputs):\n",
    "    hallucinations_reduced = sum(\n",
    "        1 for base, valid in zip(baseline_outputs, validated_outputs)\n",
    "        if base.get(\"is_hallucination\") and not valid.get(\"is_hallucination\")\n",
    "    )\n",
    "    total_hallucinations = sum(1 for base in baseline_outputs if base.get(\"is_hallucination\"))\n",
    "    return (hallucinations_reduced / total_hallucinations) * 100 if total_hallucinations > 0 else 0\n",
    "\n",
    "# Example HRR usage\n",
    "baseline_outputs = [\n",
    "    {\"query\": \"What is the boiling point of water?\", \"output\": \"50째C\", \"is_hallucination\": True},\n",
    "    {\"query\": \"Who wrote Hamlet?\", \"output\": \"Charles Dickens\", \"is_hallucination\": True}\n",
    "]\n",
    "validated_outputs = [\n",
    "    {\"query\": \"What is the boiling point of water?\", \"output\": \"100째C\", \"is_hallucination\": False},\n",
    "    {\"query\": \"Who wrote Hamlet?\", \"output\": \"William Shakespeare\", \"is_hallucination\": False}\n",
    "]\n",
    "hrr_score = calculate_hrr(baseline_outputs, validated_outputs)\n",
    "print(f\"Hallucination Reduction Rate (HRR): {hrr_score:.2f}%\")\n",
    "print(\"This score reflects the percentage of hallucinations corrected by the model. A higher value indicates fewer factual errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical Consistency Score (LCS): 100.00%\n",
      "This score measures logical reasoning accuracy. A higher value suggests the model maintains logical coherence.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Logical Consistency Score (LCS)\n",
    "def calculate_lcs(responses):\n",
    "    consistent_responses = sum(1 for response in responses if response.get(\"is_consistent\"))\n",
    "    return (consistent_responses / len(responses)) * 100\n",
    "\n",
    "# Example LCS usage\n",
    "responses = [\n",
    "    {\"query\": \"If A > B and B > C, is A > C?\", \"output\": \"Yes\", \"is_consistent\": True},\n",
    "    {\"query\": \"Is it possible for a square to have three sides?\", \"output\": \"No\", \"is_consistent\": True}\n",
    "]\n",
    "lcs_score = calculate_lcs(responses)\n",
    "print(f\"Logical Consistency Score (LCS): {lcs_score:.2f}%\")\n",
    "print(\"This score measures logical reasoning accuracy. A higher value suggests the model maintains logical coherence.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Accuracy (RA): 66.67%\n",
      "Response Accuracy measures correctness in providing factual answers. A higher value means more accurate answers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Response Accuracy (RA)\n",
    "def calculate_ra(gold_standard, model_outputs):\n",
    "    correct_responses = sum(\n",
    "        1 for gold, output in zip(gold_standard, model_outputs)\n",
    "        if gold[\"correct_answer\"] == output[\"output\"]\n",
    "    )\n",
    "    return (correct_responses / len(gold_standard)) * 100\n",
    "\n",
    "ra_score = calculate_ra(gold_standard, model_outputs)\n",
    "print(f\"Response Accuracy (RA): {ra_score:.2f}%\")\n",
    "print(\"Response Accuracy measures correctness in providing factual answers. A higher value means more accurate answers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM): True\n",
      "Exact Match evaluates if the prediction exactly matches the reference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4. Exact Match (EM)\n",
    "def exact_match(prediction, target):\n",
    "    return prediction == target\n",
    "\n",
    "em_score = exact_match(\"Paris\", \"Paris\")\n",
    "print(f\"Exact Match (EM): {em_score}\")\n",
    "print(\"Exact Match evaluates if the prediction exactly matches the reference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.80\n",
      "F1 Score balances precision and recall. Higher values suggest fewer false positives and negatives.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. F1 Score\n",
    "def calculate_f1(predictions, targets):\n",
    "    return f1_score(targets, predictions, average=\"binary\")\n",
    "\n",
    "predictions = [1, 0, 1, 1]\n",
    "targets = [1, 0, 0, 1]\n",
    "print(f\"F1 Score: {calculate_f1(predictions, targets):.2f}\")\n",
    "print(\"F1 Score balances precision and recall. Higher values suggest fewer false positives and negatives.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': Score(precision=0.8333333333333334, recall=0.8333333333333334, fmeasure=0.8333333333333334), 'rougeL': Score(precision=0.8333333333333334, recall=0.8333333333333334, fmeasure=0.8333333333333334)}\n",
      "ROUGE measures text similarity. High precision and recall suggest strong alignment with reference.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. ROUGE\n",
    "def calculate_rouge(prediction, target):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    return scorer.score(target, prediction)\n",
    "\n",
    "rouge_scores = calculate_rouge(\"The cat sat on the mat.\", \"The cat is on the mat.\")\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n",
    "print(\"ROUGE measures text similarity. High precision and recall suggest strong alignment with reference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': Score(precision=0.8333333333333334, recall=0.8333333333333334, fmeasure=0.8333333333333334), 'rougeL': Score(precision=0.8333333333333334, recall=0.8333333333333334, fmeasure=0.8333333333333334)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge(prediction, target):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    return scorer.score(target, prediction)\n",
    "\n",
    "# Example Data\n",
    "prediction = \"The cat sat on the mat.\"\n",
    "target = \"The cat is on the mat.\"\n",
    "\n",
    "# Calculate ROUGE\n",
    "rouge_scores = calculate_rouge(prediction, target)\n",
    "print(\"ROUGE Scores:\", rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.25\n",
      "BLEU evaluates translation quality. Smoothing addresses low n-gram overlap warnings.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. BLEU\n",
    "def calculate_bleu(prediction, target):\n",
    "    reference = [target.split()]\n",
    "    candidate = prediction.split()\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    return sentence_bleu(reference, candidate, smoothing_function=smoothing_function)\n",
    "\n",
    "bleu_score = calculate_bleu(\"The cat is on the mat.\", \"The cat sat on the mat.\")\n",
    "print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "print(\"BLEU evaluates translation quality. Smoothing addresses low n-gram overlap warnings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity for 'This is a friendly and respectful comment.': {'toxicity': np.float32(0.0005483509), 'severe_toxicity': np.float32(0.00014052661), 'obscene': np.float32(0.00019850969), 'threat': np.float32(0.00013926814), 'insult': np.float32(0.00018051789), 'identity_attack': np.float32(0.00014728199)}\n",
      "Toxicity for 'This is a hateful and offensive comment.': {'toxicity': np.float32(0.15707134), 'severe_toxicity': np.float32(0.00023562438), 'obscene': np.float32(0.0022060007), 'threat': np.float32(0.00058931776), 'insult': np.float32(0.0024856713), 'identity_attack': np.float32(0.0010868483)}\n",
      "Toxicity scores indicate harmful content. Lower scores are preferable for ethical AI.\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Toxicity Detection\n",
    "def detect_toxicity(text):\n",
    "    model = Detoxify('original')\n",
    "    return model.predict(text)\n",
    "\n",
    "for text in texts:\n",
    "    print(f\"Toxicity for '{text}': {detect_toxicity(text)}\")\n",
    "print(\"Toxicity scores indicate harmful content. Lower scores are preferable for ethical AI.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity Score (2-grams): 0.77\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def calculate_diversity(text, n=2):\n",
    "    words = text.split()\n",
    "    ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    total_ngrams = len(ngrams)\n",
    "    unique_ngrams = len(set(ngrams))\n",
    "    return unique_ngrams / total_ngrams if total_ngrams > 0 else 0\n",
    "\n",
    "# Example Data\n",
    "text = \"The quick brown fox jumps over the lazy dog. The quick brown fox repeats.\"\n",
    "diversity_score = calculate_diversity(text, n=2)\n",
    "print(f\"Diversity Score (2-grams): {diversity_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.72\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_coherence(context, response):\n",
    "    vectorizer = TfidfVectorizer().fit([context, response])\n",
    "    vectors = vectorizer.transform([context, response])\n",
    "    return cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "\n",
    "# Example Data\n",
    "context = \"What is the capital of France?\"\n",
    "response = \"Paris is the capital of France.\"\n",
    "coherence_score = calculate_coherence(context, response)\n",
    "print(f\"Coherence Score: {coherence_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Score: 0.12, Insult Score: 0.02\n"
     ]
    }
   ],
   "source": [
    "from detoxify import Detoxify\n",
    "\n",
    "def detect_bias(text):\n",
    "    model = Detoxify('unbiased')  # Use the 'unbiased' model\n",
    "    predictions = model.predict(text)\n",
    "    toxicity = predictions['toxicity']\n",
    "    insult = predictions['insult']\n",
    "\n",
    "    # Set thresholds (adjust as needed)\n",
    "    toxicity_threshold = 0.5 \n",
    "    insult_threshold = 0.5\n",
    "\n",
    "    if toxicity > toxicity_threshold:\n",
    "        print(\"The text is likely toxic.\")\n",
    "    if insult > insult_threshold:\n",
    "        print(\"The text is likely insulting.\")\n",
    "\n",
    "    return toxicity, insult\n",
    "\n",
    "# Example Data\n",
    "biased_text = \"Men are better leaders than women.\"\n",
    "toxicity, insult = detect_bias(biased_text)\n",
    "print(f\"Toxicity Score: {toxicity:.2f}, Insult Score: {insult:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity for 'This is a respectful comment.': {'toxicity': np.float32(0.00058519543), 'severe_toxicity': np.float32(0.00013003703), 'obscene': np.float32(0.00019055209), 'threat': np.float32(0.00012352254), 'insult': np.float32(0.00017691121), 'identity_attack': np.float32(0.00014319048)}\n",
      "Toxicity for 'This is a hateful comment.': {'toxicity': np.float32(0.12656806), 'severe_toxicity': np.float32(0.00021170401), 'obscene': np.float32(0.002147647), 'threat': np.float32(0.00049468974), 'insult': np.float32(0.0020356625), 'identity_attack': np.float32(0.00074969034)}\n"
     ]
    }
   ],
   "source": [
    "from detoxify import Detoxify\n",
    "\n",
    "def detect_toxicity(text):\n",
    "    model = Detoxify('original')\n",
    "    return model.predict(text)\n",
    "\n",
    "# Example Data\n",
    "texts = [\n",
    "    \"This is a respectful comment.\",\n",
    "    \"This is a hateful comment.\"\n",
    "]\n",
    "\n",
    "# Detect Toxicity\n",
    "for text in texts:\n",
    "    print(f\"Toxicity for '{text}': {detect_toxicity(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity for 'This is a respectful comment.':\n",
      "  Toxicity: 0.0006\n",
      "  Severe Toxicity: 0.0001\n",
      "  Obscene: 0.0002\n",
      "  Threat: 0.0001\n",
      "  Insult: 0.0002\n",
      "  Identity Attack: 0.0001\n",
      "Toxicity for 'This is a hateful comment.':\n",
      "  Toxicity: 0.1266\n",
      "  Severe Toxicity: 0.0002\n",
      "  Obscene: 0.0021\n",
      "  Threat: 0.0005\n",
      "  Insult: 0.0020\n",
      "  Identity Attack: 0.0007\n"
     ]
    }
   ],
   "source": [
    "from detoxify import Detoxify\n",
    "\n",
    "def detect_toxicity(text):\n",
    "    model = Detoxify('original')\n",
    "    return model.predict(text)\n",
    "\n",
    "# Example Data\n",
    "texts = [\n",
    "    \"This is a respectful comment.\",\n",
    "    \"This is a hateful comment.\"\n",
    "]\n",
    "\n",
    "# Detect Toxicity\n",
    "for text in texts:\n",
    "    print(f\"Toxicity for '{text}':\")\n",
    "    results = detect_toxicity(text)\n",
    "    for key, value in results.items():\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Retention Score: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_knowledge_retention(questions, correct_answers, model_outputs):\n",
    "    retained = sum(1 for q, a, o in zip(questions, correct_answers, model_outputs) if a == o)\n",
    "    return retained / len(questions) * 100\n",
    "\n",
    "# Example Data\n",
    "questions = [\"Who wrote Hamlet?\", \"What is the capital of Italy?\"]\n",
    "correct_answers = [\"William Shakespeare\", \"Rome\"]\n",
    "model_outputs = [\"William Shakespeare\", \"Rome\"]  # Outputs from the model\n",
    "knowledge_retention_score = evaluate_knowledge_retention(questions, correct_answers, model_outputs)\n",
    "print(f\"Knowledge Retention Score: {knowledge_retention_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity Score (2-grams): 0.77\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def calculate_diversity(text, n=2):\n",
    "    words = text.split()\n",
    "    ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    total_ngrams = len(ngrams)\n",
    "    unique_ngrams = len(set(ngrams))\n",
    "    return unique_ngrams / total_ngrams if total_ngrams > 0 else 0\n",
    "\n",
    "# Example Data\n",
    "text = \"The quick brown fox jumps over the lazy dog. The quick brown fox repeats.\"\n",
    "diversity_score = calculate_diversity(text, n=2)\n",
    "print(f\"Diversity Score (2-grams): {diversity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. Using `lm-evaluation-harness`\n",
    "try:\n",
    "    from lm_eval.evaluator import simple_evaluate\n",
    "    results = simple_evaluate(\n",
    "        model=\"hf\",  # Use Hugging Face AutoModel\n",
    "        model_args=\"pretrained=gpt2\",  # Specify the pretrained model\n",
    "        tasks=[\"lambada_openai\", \"piqa\"],  # Tasks to evaluate\n",
    "       #device=\"cpu\",  # Force evaluation on CPU\n",
    "    )\n",
    "    print(\"LM Evaluation Results:\", results)\n",
    "    print(\"These results summarize model performance on standard benchmarks.\")\n",
    "except ImportError as e:\n",
    "    warnings.warn(\"Ensure `lm-eval` is properly installed for this step.\")\n",
    "except AssertionError as e:\n",
    "    print(\"Error:\", e)\n",
    "    print(\"Ensure your PyTorch installation matches your hardware capabilities (CPU or GPU).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
