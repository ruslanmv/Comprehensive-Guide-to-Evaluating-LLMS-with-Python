{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination Reduction Rate (HRR): 100.00%\n",
      "Logical Consistency Score (LCS): 100.00%\n",
      "Response Accuracy (RA): 66.67%\n",
      "Exact Match (EM): True\n",
      "F1 Score: 0.80\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.8333333333333334, recall=0.8333333333333334, fmeasure=0.8333333333333334), 'rougeL': Score(precision=0.8333333333333334, recall=0.8333333333333334, fmeasure=0.8333333333333334)}\n",
      "BLEU Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dropbox\\23-GITHUB\\Projects\\Comprehensive-Guide-to-Evaluating-LLMS-with-Python\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Dropbox\\23-GITHUB\\Projects\\Comprehensive-Guide-to-Evaluating-LLMS-with-Python\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: \"https://github.com/unitaryai/detoxify/releases/download/v0.1-alpha/toxic_original-c1212f89.ckpt\" to C:\\Users\\rusla/.cache\\torch\\hub\\checkpoints\\toxic_original-c1212f89.ckpt\n",
      "100%|██████████| 418M/418M [00:14<00:00, 31.2MB/s] \n",
      "c:\\Dropbox\\23-GITHUB\\Projects\\Comprehensive-Guide-to-Evaluating-LLMS-with-Python\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rusla\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity for 'This is a friendly and respectful comment.': {'toxicity': np.float32(0.0005483509), 'severe_toxicity': np.float32(0.00014052661), 'obscene': np.float32(0.00019850969), 'threat': np.float32(0.00013926814), 'insult': np.float32(0.00018051789), 'identity_attack': np.float32(0.00014728199)}\n",
      "Toxicity for 'This is a hateful and offensive comment.': {'toxicity': np.float32(0.15707134), 'severe_toxicity': np.float32(0.00023562438), 'obscene': np.float32(0.0022060007), 'threat': np.float32(0.00058931776), 'insult': np.float32(0.0024856713), 'identity_attack': np.float32(0.0010868483)}\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Evaluator' from 'lm_eval' (c:\\Dropbox\\23-GITHUB\\Projects\\Comprehensive-Guide-to-Evaluating-LLMS-with-Python\\.venv\\Lib\\site-packages\\lm_eval\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 149\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToxicity for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetect_toxicity(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# ---\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# ### 9. Using `lm-evaluation-harness`\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Install this package if you haven't:\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# !pip install lm-eval\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlm_eval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Evaluator\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Basic Usage of lm-evaluation-harness\u001b[39;00m\n\u001b[0;32m    152\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m, tasks\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambada\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpiqa\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Evaluator' from 'lm_eval' (c:\\Dropbox\\23-GITHUB\\Projects\\Comprehensive-Guide-to-Evaluating-LLMS-with-Python\\.venv\\Lib\\site-packages\\lm_eval\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Comprehensive Guide to Evaluating Language Models (LLMs) with Python\n",
    "# ## Introduction\n",
    "# \n",
    "# Evaluating the performance of Large Language Models (LLMs) is an essential step in ensuring they meet user expectations for accuracy, logical reasoning, ethical behavior, and usability. This comprehensive guide combines theoretical understanding, formulas, and Python implementations for a wide array of metrics. By the end of this notebook, you will have all the tools necessary to benchmark and improve your LLM.\n",
    "\n",
    "# ---\n",
    "# ## Setting Up the Environment\n",
    "# Install all required libraries if not already installed\n",
    "# Uncomment and run the following lines if you haven't installed the required packages.\n",
    "\n",
    "# !pip install numpy pandas sklearn rouge-score nltk detoxify lm-eval matplotlib\n",
    "\n",
    "# ### Example Datasets\n",
    "# Let's define example datasets that will be used across multiple metric evaluations.\n",
    "\n",
    "# Example dataset for accuracy and logical consistency\n",
    "gold_standard = [\n",
    "    {\"query\": \"What is 2 + 2?\", \"correct_answer\": \"4\"},\n",
    "    {\"query\": \"Who wrote Macbeth?\", \"correct_answer\": \"William Shakespeare\"},\n",
    "    {\"query\": \"What is the boiling point of water?\", \"correct_answer\": \"100°C\"}\n",
    "]\n",
    "\n",
    "model_outputs = [\n",
    "    {\"query\": \"What is 2 + 2?\", \"output\": \"4\"},\n",
    "    {\"query\": \"Who wrote Macbeth?\", \"output\": \"Charles Dickens\"},\n",
    "    {\"query\": \"What is the boiling point of water?\", \"output\": \"100°C\"}\n",
    "]\n",
    "\n",
    "# Example dataset for toxicity detection\n",
    "texts = [\n",
    "    \"This is a friendly and respectful comment.\",\n",
    "    \"This is a hateful and offensive comment.\"\n",
    "]\n",
    "\n",
    "# ---\n",
    "# ## Metrics and Python Implementations\n",
    "\n",
    "# ### 1. Hallucination Reduction Rate (HRR)\n",
    "def calculate_hrr(baseline_outputs, validated_outputs):\n",
    "    hallucinations_reduced = sum(\n",
    "        1 for base, valid in zip(baseline_outputs, validated_outputs)\n",
    "        if base[\"is_hallucination\"] and not valid[\"is_hallucination\"]\n",
    "    )\n",
    "    total_hallucinations = sum(1 for base in baseline_outputs if base[\"is_hallucination\"])\n",
    "    hrr = (hallucinations_reduced / total_hallucinations) * 100 if total_hallucinations > 0 else 0\n",
    "    return hrr\n",
    "\n",
    "# Example usage for HRR\n",
    "baseline_outputs = [\n",
    "    {\"query\": \"What is the boiling point of water?\", \"output\": \"50°C\", \"is_hallucination\": True},\n",
    "    {\"query\": \"Who wrote Hamlet?\", \"output\": \"Charles Dickens\", \"is_hallucination\": True}\n",
    "]\n",
    "validated_outputs = [\n",
    "    {\"query\": \"What is the boiling point of water?\", \"output\": \"100°C\", \"is_hallucination\": False},\n",
    "    {\"query\": \"Who wrote Hamlet?\", \"output\": \"William Shakespeare\", \"is_hallucination\": False}\n",
    "]\n",
    "hrr_score = calculate_hrr(baseline_outputs, validated_outputs)\n",
    "print(f\"Hallucination Reduction Rate (HRR): {hrr_score:.2f}%\")\n",
    "\n",
    "# ---\n",
    "# ### 2. Logical Consistency Score (LCS)\n",
    "def calculate_lcs(responses):\n",
    "    consistent_responses = sum(1 for response in responses if response[\"is_consistent\"])\n",
    "    return (consistent_responses / len(responses)) * 100\n",
    "\n",
    "# Example usage for LCS\n",
    "responses = [\n",
    "    {\"query\": \"If A > B and B > C, is A > C?\", \"output\": \"Yes\", \"is_consistent\": True},\n",
    "    {\"query\": \"Is it possible for a square to have three sides?\", \"output\": \"No\", \"is_consistent\": True}\n",
    "]\n",
    "lcs_score = calculate_lcs(responses)\n",
    "print(f\"Logical Consistency Score (LCS): {lcs_score:.2f}%\")\n",
    "\n",
    "# ---\n",
    "# ### 3. Response Accuracy (RA)\n",
    "def calculate_ra(gold_standard, model_outputs):\n",
    "    correct_responses = sum(\n",
    "        1 for gold, output in zip(gold_standard, model_outputs)\n",
    "        if gold[\"correct_answer\"] == output[\"output\"]\n",
    "    )\n",
    "    return (correct_responses / len(gold_standard)) * 100\n",
    "\n",
    "# Example usage for RA\n",
    "ra_score = calculate_ra(gold_standard, model_outputs)\n",
    "print(f\"Response Accuracy (RA): {ra_score:.2f}%\")\n",
    "\n",
    "# ---\n",
    "# ### 4. Exact Match (EM)\n",
    "def exact_match(prediction, target):\n",
    "    return prediction == target\n",
    "\n",
    "# Example usage for EM\n",
    "em_score = exact_match(\"Paris\", \"Paris\")\n",
    "print(f\"Exact Match (EM): {em_score}\")\n",
    "\n",
    "# ---\n",
    "# ### 5. F1 Score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_f1(predictions, targets):\n",
    "    return f1_score(targets, predictions, average=\"binary\")\n",
    "\n",
    "# Example usage for F1\n",
    "predictions = [1, 0, 1, 1]\n",
    "targets = [1, 0, 0, 1]\n",
    "print(f\"F1 Score: {calculate_f1(predictions, targets):.2f}\")\n",
    "\n",
    "# ---\n",
    "# ### 6. ROUGE\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge(prediction, target):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    return scorer.score(target, prediction)\n",
    "\n",
    "# Example usage for ROUGE\n",
    "rouge_scores = calculate_rouge(\"The cat sat on the mat.\", \"The cat is on the mat.\")\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n",
    "\n",
    "# ---\n",
    "# ### 7. BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_bleu(prediction, target):\n",
    "    reference = [target.split()]\n",
    "    candidate = prediction.split()\n",
    "    return sentence_bleu(reference, candidate)\n",
    "\n",
    "# Example usage for BLEU\n",
    "bleu_score = calculate_bleu(\"The cat is on the mat.\", \"The cat sat on the mat.\")\n",
    "print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "\n",
    "# ---\n",
    "# ### 8. Toxicity Detection\n",
    "from detoxify import Detoxify\n",
    "\n",
    "def detect_toxicity(text):\n",
    "    model = Detoxify('original')\n",
    "    return model.predict(text)\n",
    "\n",
    "# Example usage for Toxicity Detection\n",
    "for text in texts:\n",
    "    print(f\"Toxicity for '{text}': {detect_toxicity(text)}\")\n",
    "\n",
    "# ---\n",
    "# ### 9. Using `lm-evaluation-harness`\n",
    "# Install this package if you haven't:\n",
    "# !pip install lm-eval\n",
    "from lm_eval import Evaluator\n",
    "\n",
    "# Basic Usage of lm-evaluation-harness\n",
    "evaluator = Evaluator(model=\"gpt2\", tasks=[\"lambada\", \"piqa\"])\n",
    "results = evaluator.evaluate()\n",
    "print(\"LM Evaluation Results:\", results)\n",
    "\n",
    "# ---\n",
    "# ## Final Thoughts\n",
    "# This notebook demonstrated how to evaluate LLMs using Python. By applying these metrics to real-world datasets, you can gain deeper insights into model performance and identify areas for improvement.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
